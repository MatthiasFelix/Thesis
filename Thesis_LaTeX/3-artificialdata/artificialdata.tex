\chapter{Artificial Data Generation}
\label{c:artificialdatageneration}
% NETWORK GENERATION
\section{Artificial Network Generation}
\label{st:artificialnetworkgeneration} There has been a decent amount of studies that have analyzed the performance of social recommender algorithms with the help of real-world data. Examples include \cite{Groh_2007} who analyzed the social network of the german community Lokalisten \cite{Lokalisten}, or \cite{Liu_2010} who distributed a survey to users of Cyworld (a popular South Korean social network site) \cite{Cyworld}. But obtaining good and complete real-world data can be a difficult task. Online social networking sites want to provide a certain amount of privacy for their users and they want users to trust them with their data. Additionally, there are also national laws concerning data protection and security. For those reasons, it is often difficult to study real-world examples of social recommender systems.

While there is one suitable dataset from the music streaming platform last.fm \cite{Lastfm} that will be used in this thesis, it is also desirable to have artificial datasets with adjustable parameters in order to study the performance of social recommender algorithms in different settings. Therefore a model needs to be found that constructs graphs with properties of a social network (see section \ref{sst:socialnetworkcharacteristics}).

Many network generation algorithms and models exist, a good overview can be found in \cite{Zaidi_2012}. The problem is that most of these algorithms produce networks with one or the other property, but fail to produce networks that have all the characteristics of a social network (including community structure). The authors of this paper therefore not only compare existing algorithms, but also propose a new algorithm which seems to produce networks similar to real-world ones, showing all desirable properties. The algorithm works as follows:

Social networks consist of groups of densely connected users, called communities, or sometimes even cliques (section \ref{sst:socialnetworkcharacteristics}). The first step of the algorithm is therefore to create cliques of various sizes. These will be the ``building blocks'' of the network. Adding cliques will give the network a high clustering coefficient, since many triangles are formed (a clique, no matter how many nodes it contains, always has a clustering coefficient of 1). A high clustering coefficient indicates that there is a clear community structure in the network.

As a next step, the cliques have to be connected to form one big network. In real networks, there also exist connecting points between different communities. A person might belong to a strongly connected community of friends and family and also to a community consisting of people from work. This person figures as a connecting point of those two (otherwise probably not connected) communities. The way these connections are achieved is to assign each node an ``open connections'' attribute which determines how many additional connections it can make. This correlates with the degree of the node, which is why we have to draw the number of open connections from a power law distribution. This results in few nodes having a very high degree and many nodes having a low degree (scale-freeness). Finally, the nodes are merged. Two nodes that still have open connections are randomly selected and merged into one, resulting in two connected communities. The open connection attribute of both nodes is lowered by 1 and then randomly one of the attributes of the two old nodes is assigned to the new node. This is done until there are no more remaining open connections.
\newline

Formally, the different steps of the algorithm are the following:

\begin{enumerate}
\item The algorithm takes as input parameters a number of cliques $k$ to be generated in the beginning. The cliques will have sizes between $minSize$ and $maxSize$.
\item Generate k cliques, each with a size between $minSize$ and $maxSize$, following an exponential distribution $p(x)\:dx = \frac{1}{\mu} \cdot e^{\frac{-x}{\mu}}\:dx$, with $\mu = 2$.
\item Each node receives an ``open connections'' attribute, drawn from an exponential distribution $p(x)\:dx = \frac{1}{\mu} \cdot e^{\frac{-x}{\mu}}\:dx$ with $\mu$ as input parameter.
\item While there are open connections, randomly draw two nodes that are not yet connected and that both still have at least one open connection. Merge the two nodes into one, lower both ``open connection'' attributes by 1 and randomly assign the new node one of the two attributes.
\end{enumerate}

\subsection{Community Structure Detecting Algorithms}
\label{sst:communityalgorithms} As described in section \ref{sst:socialnetworkcharacteristics}, social networks often reveal some sort of community structure, meaning there are groups of densely connected nodes and few connections between those groups. However, there is no mathematical definition for communities. Nevertheless, there are different approaches to find community structure, of which the most important ones will be presented here.

To be able to compare the quality of a particular division of a network into communities, a measure called \textit{modularity} that was introduced by \cite{Newman_2006} can be used. It measures the fraction of edges that are within the given communities minus the expected fraction of edges that would have been within the given communities if edges were distributed at random (keeping node degrees fixed). 

To explain the calculation of the modularity of a given network division, we first consider the simple case of a division into 2 communities. For a graph with $n$ nodes and $m$ edges, we define a membership vector $s$, with $s_v = 1$ if node $v$ belongs to community 1, and $s_v = -1$ if node $v$ belongs to community 2. We further need the adjacency matrix $A$, with $A_{vw} = 1$ meaning there exists an edge between nodes $v$ and $w$, and $A_{vw} = 0$ meaning there is no edge. To calculate the expected number of edges between two nodes, we need a null-model where the degree of each node is kept but the edges are randomly rewired. The expected number of edges between nodes $v$ and $w$ is then $\frac{d_v d_w}{2m}$, with $d_v$ being the degree of node $v$ and $2m$ being the total number of rewiring possibilities. The actual number of edges between two nodes is just $A_{vw}$, either 0 or 1. We include the factor $\frac{s_v s_w  + 1}{2}$, which is 1 if $v$ and $w$ are in the same community and 0 otherwise, which gives us the formula for modularity $Q$:

\begin{equation}
Q = \frac{1}{2m} \sum_{vw}(A_{vw} - \frac{d_v d_w}{2m}) \frac{s_v s_w  + 1}{2}
\label{eq:modularity}
\end{equation}

The division of the whole term by $2m$ is merely conventional, so the value of the modularity score is in the range [-1/2,1). The extension of this formula to the case of more than two communities is trivial and can be made by replacing the factor $\frac{s_v s_w  + 1}{2}$ with a function $\delta(c_v, c_w)$ that is 1 if $c_v = c_w$ (that is, nodes $v$ and $w$ are in the same community) and 0 otherwise.

\subsubsection{Minimum-Cut}
\label{ssst:minimumcut} This is one of the oldest and simplest algorithms to partition a network into communities. It cuts the network into a fixed number of groups, usually of approximately the same size, minimizing the number of edges between the groups. There exist application areas where this approach works quite well (e.g. load balancing  for parallel computing), but it is clearly not well-suited for detecting the communities in a social network, since it always finds a fixed number of communities (all of approximately the same size), regardless if they really exist in the network or not. In social networks, often communities of different sizes will have to be detected and the number of communities is unknown a priori.

\subsubsection{Greedy Modularity Optimization}
\label{ssst:modularityoptimization} The most popular community detection methods use some form of modularity optimization. Exhaustive optimization of modularity has been proven to be an NP-complete problem \cite{Brandes_2006}, thus in practice other, greedy optimization methods must be used. One of the most popular approaches using modularity optimization is the one proposed by \cite{Clauset_2004}. It belongs to the class of agglomerative methods. Methods of this class start with all vertices being in $n$ seperate communities, and then repeatedly join communities until all vertices belong to one community. This results in a so-called \textit{dendrogram}, where cuts on different layers represent different divisions of the network into communities.

The algorithm of \cite{Clauset_2004} now joins at each step the two communities that result in the biggest increase (or smallest decrease) in modularity $Q$. At each step, a division with a certain value of $Q$ is obtained and it is then trivial to choose the division with the highest value of $Q$. This algorithm runs in time $O((m+n)n)$, or $O(n^2)$ on sparse graphs (which social networks often are).

\subsubsection{Edge Betweenness}
\label{ssst:edgebetweenness} An algorithm that detects community structure with the help of edge betweenness (see section \ref{sst:graphproperties}) was proposed by \cite{Girvan_2002}. In contrast to the agglomerative algorithm of \cite{Clauset_2004} described above, this is a divisive algorithm. It starts with one single community containing all nodes and then gradually removes edges, which also results in a dendrogram revealing the community structure of the network. The algorithm is quite simple:

\begin{enumerate}
\item Calculate the betweenness for all edges in the network.
\item Remove the edge with the highest betweenness.
\item Recalculate the betweenness for all edges that were affected by the removal.
\item Repeat steps 2 and 3 until no edges remain.
\end{enumerate}

The resulting divisions, given by cuts through different levels of the dendrogram, can then easily be scanned to find the one with the highest modularity score.

The big disadvantage of this algorithm is that is runs in worst-case time $O(m^2n)$. This makes it unsuitable for large social networks. In our case (networks with a few thousand nodes), the edge-betweenness algorithm already takes impracticably much time.

\subsubsection{Community Walktrap}
\label{ssst:communitywalktrap} This is an algorithm proposed by \cite{Pons_2005}. Their approach is based on the assumption that random walks on networks tend to stay within communities, since there are many connections inside communities and only few between them. It runs in worst-case time $O(m n^2)$, but since most real-world networks are very sparse, it usually runs in $O(n^2\log n)$.

\cite{Pons_2005} introduce a distance measure $r$ between two nodes, which is large if the nodes are in different communities and small otherwise. This distance is computed from the information given by random walks in the graph. The algorithm itself then is also an agglomerative one; it starts with $n$ communities containing one node each, and then merges communities based on their distance. At each step, the two communities are merged that minimize the mean of the squared distances between each node and its community.
% RATING GENERATION
\section{Artificial Rating Generation}
\label{st:artificialratinggeneration} The generation of artificial ratings is a discipline that has not been in the focus of recent research like network generation. The application area of artificially generated ratings is very limited and mostly constricted to empiric research and experiments. Because of the lack of existing models, an appropriate model to generate artificial rating data was created and implemented. In the next section, the design choices that were made are explained and justified in more detail.

\subsection{Design Choices}
\label{sst:designchoices} First, a look at the existing dataset from last.fm that is used in this thesis shall be taken. The rating set contains 92'834 ratings of 1'892 users on 17'632 artists. The rating distribution (figure \ref{f:frequencyofratings}) is very skewed to the left, indicating that users give most items a rather low rating and only few items a very good rating. The outlier at rating 5 is due to the normalization process which is done in a way that normalizes each user's personal favorite artist to a rating of 5.

\begin{figure}[!ht]
\centering
\includegraphics[width=325px]{./3-artificialdata/figures/FrequencyOfRatings_Lastfm.png}
\caption{Frequency of ratings of the last.fm dataset}
\label{f:frequencyofratings}
\end{figure}

Since it is assumed and to be shown that there exists a certain correlation between friends' ratings, this also has to be taken into account for the rating generation. Varying the degree of correlation will help to show if social network information can increase collaborative filtering performance. Further, to create a realistic rating generation model, there needs to be some sort of quality differences between items, as well as different user and community tastes. For the algorithm, we will draw those three factors from an exponential distribution. It is realistic to assume that there are very few items of very high quality and a lot of low quality items, and the same goes for the tastes of individuals and groups.

Furthermore, the number of ratings each user provides is assumed to follow a normal distribution. This distribution choice can not be verified or falsified in the last.fm dataset, since this set has been arranged so that almost every user has given 50 ratings.

\subsection{The Algorithm}
\label{sst:thealgorithm} The rating generation algorithm takes as starting point a previously generated social network (generated using the algorithm presented in section \ref{st:artificialnetworkgeneration}) or any existing social network and a community division, resulting from applying a community detection algorithm (see section \ref{sst:communityalgorithms}) that places every user in a single community. Once the number of users $n$ and the $k$ communities they belong to are known, every user $i$ is given a taste $n_i$ drawn from an exponential distribution. Let there be a total number of $t$ items, and every item $j$ is also given a quality $t_j$ drawn from an exponential distribution, as well as every community $c$ is given a community taste $k_c$. All these tastes are afterwards normalized to be in the range the creator wants the ratings to be (often $[0,1]$ or $[1,5]$).

Before ratings are created, every user $i$ also receives a number of ratings $l_i$ he will give, which is drawn from a normal distribution. The mean and standard deviation of this normal distribution can be seen as parameters; they directly determine how sparse or how dense the resulting rating matrix will be.

As it can be assumed that not only the ratings of users in the same community correlate, but also the set of items they have rated, each community also receives a community item set $Q$ with $q$ items, drawn randomly from the whole item set. The total number of items $t$ is a parameter of the algorithm, as well as $q$. 

The next step is to create for every user the specified number of ratings. User $i$'s rating on item $j$ depends on 3 factors: item $j$'s quality, user $i$'s personal taste and user $i$'s community's taste. The ratings are created with the following procedure:

\begin{enumerate}
\item Choose a user $i$.
\item Randomly choose an item $j$. With probability $p$, choose it from the whole item set, and with probability $1-p$, choose it from user $i$'s community's item set.
\item If the user has not already rated this item, calculate $r_i(j)$ as $a \cdot t_j + b \cdot n_i + c \cdot k_c$, with $a,b,c \in [0,1]$ and $a+b+c=1$. $r_i(j)$ is thus a convex combination of the three factors $t_j$, $n_i$ and $k_c$.
\item Repeat steps 2 and 3 until user $i$ has $l_i$ ratings.
\item Repeat steps 1 to 4 for all users.
\end{enumerate}